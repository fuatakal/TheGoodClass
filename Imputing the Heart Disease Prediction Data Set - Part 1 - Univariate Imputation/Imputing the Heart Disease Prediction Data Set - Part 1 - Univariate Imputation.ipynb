{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Imputation on the Heart Disease Prediction Data Set\n",
    "\n",
    "#### by Fuat Akal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "[Problem](#problem)  \n",
    "[Loading Libraries](#loading_libraries)  \n",
    "[Data Preparation](#data_preparation)   \n",
    "[Imputation](#imputation)   \n",
    "[Discussion](#discussion)   \n",
    "[References](#references)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem <a class=\"anchor\" id=\"problem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various reasons, many real world datasets from the healthcare domain contain missing values, often encoded as blanks, NaNs or other placeholders (e.g., Not checked). Honestly, this is understandable. Because, such data is collected in a clinic environment, which is very busy and stressful. On the other hand, scikit-learn estimators assume that all values in a dataset ( I mean a numpy array) are numerical, and that all have and hold meaning. \n",
    "\n",
    "A simple way to handle missing data is to remove rows or columns with missing values. This approach, however, may result in data lost. \n",
    "\n",
    "In this small data science project, we will explore our alternatives for completing (impute) missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries <a class=\"anchor\" id=\"loading_libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configure Constants\n",
    "seed = 42 # ultimate answer to everything\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None # default 60\n",
    "\n",
    "columnToImpute = \"Cholesterol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation<a class=\"anchor\" id=\"data_preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Heart Disease Prediction Data Set is available at Kaggle [1].\n",
    "\n",
    "It is downloaded and put under a local folder for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve data from a local folder\n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "\n",
    "# Display top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 12)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               0\n",
       "Sex               0\n",
       "ChestPainType     0\n",
       "RestingBP         0\n",
       "Cholesterol       0\n",
       "FastingBS         0\n",
       "RestingECG        0\n",
       "MaxHR             0\n",
       "ExerciseAngina    0\n",
       "Oldpeak           0\n",
       "ST_Slope          0\n",
       "HeartDisease      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if there is any missing value in the dataset.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was going to create 5% missing value on the Cholesterol column. Actually, I did and then performed my analyses. However, I later discovered that the dataset is not really complete. There are just too many zeros on the Cholesterol column. So, I decided to impute zeros directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is complete. So, we have to remove some to demonstrate imputation.\n",
    "# Let's pick a continuous value: Cholesterol\n",
    "# We will remove 5% of the values on that column.\n",
    "# df.loc[df.sample(frac=0.05).index, columnToImpute] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      172\n",
       "254     11\n",
       "223     10\n",
       "220     10\n",
       "211      9\n",
       "Name: Cholesterol, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will consider zero\"s pn the Cholesterol column as missing\n",
    "# Missing value rate = 172/918 = 0.187\n",
    "df['Cholesterol'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strategies.\n",
    "# Actually, there is another one called \"constant\" that I do not consider here.\n",
    "# Because, I do not have any expertise about what that constant might be.\n",
    "strategies = ['drop', 'mean', 'median', 'most_frequent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                 int64\n",
       "Sex                object\n",
       "ChestPainType      object\n",
       "RestingBP           int64\n",
       "Cholesterol         int64\n",
       "FastingBS           int64\n",
       "RestingECG         object\n",
       "MaxHR               int64\n",
       "ExerciseAngina     object\n",
       "Oldpeak           float64\n",
       "ST_Slope           object\n",
       "HeartDisease        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are categorical columns in the data set.\n",
    "# We should convert them to numbers.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of categorical columns\n",
    "categoricalColumns = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies for categorical columns\n",
    "dfDummied = pd.get_dummies(df, columns=categoricalColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation<a class=\"anchor\" id=\"imputation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cros-validation scores for different imputation strategies\n",
      "\n",
      "Strategy          Score   Std.Dev.\n",
      "-----------------------------------\n",
      "drop              0.941    0.030\n",
      "mean              0.945    0.027\n",
      "median            0.943    0.028\n",
      "most_frequent     0.944    0.024\n"
     ]
    }
   ],
   "source": [
    "# At this part, we will impute data by using different imputation strategies\n",
    "# and then, run a classifier on the imputed data to find training scores.\n",
    "# Our goal is to determine which strategy will work best for the classification.\n",
    "\n",
    "print(\"Cros-validation scores for different imputation strategies\\n\")\n",
    "print(\"Strategy          Score   Std.Dev.\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Perform imputation for each strategy and \n",
    "# apply random forest classification to imputed data.\n",
    "for s in strategies:\n",
    "    \n",
    "    \n",
    "    # We randomly picked this classifier\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    dfTemp = dfDummied.copy()\n",
    "\n",
    "    if s == 'drop':\n",
    "        # We will not impute\n",
    "        # We will remove rows with missing values instead.\n",
    "        dfTemp = dfTemp.dropna(axis=0)\n",
    "    else:\n",
    "        # Set the strategy\n",
    "        imp = SimpleImputer(missing_values=0, strategy=s)\n",
    "        dfTemp[columnToImpute] = imp.fit_transform(dfTemp[[columnToImpute]])\n",
    "    \n",
    "    # Create independent and dependant variables sets\n",
    "    X, y = dfTemp.values[:, :11], dfTemp.values[:, 11]\n",
    "    \n",
    "    # Let's perform a 10-fold cross validation\n",
    "    scores = cross_val_score(rf, X, y, cv=10)\n",
    "    \n",
    "    # store the scores in a list\n",
    "    results.append(scores)\n",
    "    \n",
    "    print('%-15s %7.3f  %7.3f' % (s, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the entire thing by using a bigger missing value rate, e.g. 25%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Strategy          Score   Std.Dev.\n",
    "-----------------------------------\n",
    "drop              0.939    0.039\n",
    "mean              0.945    0.027\n",
    "median            0.945    0.027\n",
    "most_frequent     0.948    0.026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion<a class=\"anchor\" id=\"discussion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, I can not say for sure that the univariate imputation worked well for this dataset and for the selected column when the missing value rate is relatively low (18% in my experiments). \n",
    "\n",
    "However, I guess I can say that imputation is likely to work better than dropping rows when the missing value rate is getting higher. Of course, if that value is too much high, it may be better to consider dropping that column.\n",
    "\n",
    "I do not know. There are just too many possibilities to try. Depending on the dataset's characteristics, mean and most_frequent strategies seem more likely to work for me. \n",
    "\n",
    "On the other hand, most of the cases, the univariate imputation is a no go for me. After all, if a patient's cholesterol was not measured, it makes no sense to put a random number instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Heart Failure Prediction Dataset](https://www.kaggle.com/fedesoriano/heart-failure-prediction).\n",
    "2. [Statistical Imputation for Missing Values in Machine Learning](https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Disclaimer!** This notebook is available for educational purposes only. There is no guarantee on the correctness of the content provided.\n",
    "\n",
    "If you think there is any copyright violation, please let me [know](https://forms.gle/BNNRB2kR8ZHVEREq8). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
