{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Imputation on the Heart Disease Prediction Data Set\n",
    "\n",
    "#### by Fuat Akal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "[Problem](#problem)  \n",
    "[Loading Libraries](#loading_libraries)  \n",
    "[Data Preparation](#data_preparation)   \n",
    "[Imputation](#imputation)   \n",
    "[Discussion](#discussion)   \n",
    "[References](#references)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem <a class=\"anchor\" id=\"problem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various reasons, many real world datasets from the healthcare domain contain missing values, often encoded as blanks, NaNs or other placeholders (e.g., Not checked). Honestly, this is understandable. Because, such data is collected in a clinic environment, which is very busy and stressful. On the other hand, scikit-learn estimators assume that all values in a dataset ( I mean a numpy array) are numerical, and that all have and hold meaning. \n",
    "\n",
    "A simple way to handle missing data is to remove rows or columns with missing values. This approach, however, may result in data lost. \n",
    "\n",
    "One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer) [1].\n",
    "\n",
    "In this small data science project, we will explore our alternatives for completing (impute) missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries <a class=\"anchor\" id=\"loading_libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configure Constants\n",
    "seed = 42 # ultimate answer to everything\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None # default 60\n",
    "\n",
    "columnsToImpute = [\"Cholesterol\", \"MaxHR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation<a class=\"anchor\" id=\"data_preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Heart Disease Prediction Data Set is available at Kaggle [2].\n",
    "\n",
    "I downloaded and put it under a local folder for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve data from a local folder\n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "\n",
    "# Display top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 12)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will impute two columns. But, I will use one of them for comparisons.\n",
    "imputedDF = pd.DataFrame()\n",
    "imputedDF.insert(0, 'original', df[columnsToImpute[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is complete. So, we have to remove some to demonstrate imputation.\n",
    "# Let's pick a continuous value: MaxHR\n",
    "# We will remove 5% of the values on that columns\n",
    "df.loc[df.sample(frac=0.05).index, columnsToImpute[1]] = np.nan\n",
    "\n",
    "# I noticed there are too many zeros on the Cholesterol column.\n",
    "# Missing values were probably encoded as zeros.\n",
    "# I will also impute them. But, first will convert them to np.NANs.\n",
    "# Not a necessary step. I just feel like it.\n",
    "df['Cholesterol'] = df['Cholesterol'].replace({0:np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                 0\n",
       "Sex                 0\n",
       "ChestPainType       0\n",
       "RestingBP           0\n",
       "Cholesterol       172\n",
       "FastingBS           0\n",
       "RestingECG          0\n",
       "MaxHR              46\n",
       "ExerciseAngina      0\n",
       "Oldpeak             0\n",
       "ST_Slope            0\n",
       "HeartDisease        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it worked. \n",
    "# Cholesterol: 172 missing values. Nearly 19% of 918.\n",
    "# MaxHR: 46 missing values. 5% of 918.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                 int64\n",
       "Sex                object\n",
       "ChestPainType      object\n",
       "RestingBP           int64\n",
       "Cholesterol       float64\n",
       "FastingBS           int64\n",
       "RestingECG         object\n",
       "MaxHR             float64\n",
       "ExerciseAngina     object\n",
       "Oldpeak           float64\n",
       "ST_Slope           object\n",
       "HeartDisease        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are categorical columns in the data set.\n",
    "# We should convert them to numbers.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of categorical columns\n",
    "categoricalColumns = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies for categorical columns\n",
    "dfDummied = pd.get_dummies(df, columns=categoricalColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation<a class=\"anchor\" id=\"imputation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strategies.\n",
    "# Actually, there is another one called \"constant\" that I do not consider here.\n",
    "# Because, I do not have any expertise about what that constant might be.\n",
    "# Iterative means multivariate imputation.\n",
    "strategies = ['drop', 'mean', 'median', 'most_frequent', 'iterative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cros-validation scores for different imputation strategies\n",
      "\n",
      "Strategy          Score   Std.Dev.\n",
      "-----------------------------------\n",
      "drop              0.935    0.035\n",
      "mean              0.950    0.027\n",
      "median            0.943    0.025\n",
      "most_frequent     0.945    0.029\n",
      "iterative         0.949    0.027\n"
     ]
    }
   ],
   "source": [
    "# At this part, we will impute data by using different imputation strategies\n",
    "# and then, run a classifier on the imputed data to find training scores.\n",
    "# Our goal is to determine which strategy will work best for the classification.\n",
    "\n",
    "print(\"Cros-validation scores for different imputation strategies\\n\")\n",
    "print(\"Strategy          Score   Std.Dev.\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "results = []\n",
    "\n",
    "imputedDF.insert(0, 'missing', df[columnsToImpute[1]])\n",
    "\n",
    "imputedList.append(dfDummied[columnsToImpute[1]])\n",
    "\n",
    "# Perform imputation for each strategy and \n",
    "# apply random forest classification to imputed data.\n",
    "for s in strategies:\n",
    "        \n",
    "    # We randomly picked this classifier\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    dfTemp = dfDummied.copy()\n",
    "\n",
    "    if s == 'drop':\n",
    "        # We will not impute\n",
    "        # We will remove rows with missing values instead.\n",
    "        dfTemp = dfTemp.dropna(axis=0)\n",
    "    elif s == 'iterative':\n",
    "        # Means multivariate imputation\n",
    "        imp = IterativeImputer(missing_values=np.NaN, random_state=seed)\n",
    "        dfTemp = pd.DataFrame(imp.fit_transform(dfTemp), columns=dfTemp.columns)\n",
    "        imputedDF.insert(0, s, dfTemp[columnsToImpute[1]])\n",
    "\n",
    "    else:\n",
    "        # Set the strategy for un≈üvariate imputation\n",
    "        imp = SimpleImputer(missing_values=np.NaN, strategy=s)\n",
    "        dfTemp[columnsToImpute] = imp.fit_transform(dfTemp[columnsToImpute])\n",
    "        imputedDF.insert(0, s, dfTemp[columnsToImpute[1]])\n",
    "    \n",
    "    \n",
    "    # Create independent and dependant variables sets\n",
    "    X, y = dfTemp.values[:, :11], dfTemp.values[:, 11]\n",
    "    \n",
    "    # Let's perform a 10-fold cross validation\n",
    "    scores = cross_val_score(rf, X, y, cv=10)\n",
    "    \n",
    "    # store the scores in a list\n",
    "    results.append(scores)\n",
    "    \n",
    "    print('%-15s %7.3f  %7.3f' % (s, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no surprise for the drop column case. The less data you have, worse results you get.\n",
    "\n",
    "On the hand, when we compare the RF classification results, we see that the multivariate (iterative) imputing did not achieve better than its univariate rivals, either.\n",
    "\n",
    "So, I can not say that multivariate is better than univariate, yet. We should dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputedDF.to_csv(\"data/imputedDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterative</th>\n",
       "      <th>most_frequent</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>missing</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>123.389351</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>136.508028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>159.560506</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>136.508028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>130.549229</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>136.508028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>152.985189</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>136.508028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>149.873344</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>136.508028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iterative  most_frequent  median        mean  missing  original\n",
       "18  123.389351          150.0   138.0  136.508028      NaN       125\n",
       "25  159.560506          150.0   138.0  136.508028      NaN       178\n",
       "32  130.549229          150.0   138.0  136.508028      NaN       122\n",
       "38  152.985189          150.0   138.0  136.508028      NaN       148\n",
       "40  149.873344          150.0   138.0  136.508028      NaN       130"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see how imputation worked.\n",
    "# Remember, these are original and imputed values for the MaxHR column.\n",
    "# The first line reads as: The original value was 125. It was randomly set to np.NAN and then imputed.\n",
    "# The iterative imputer returned the closest value, i.e., 123.389351.\n",
    "imputedDF[imputedDF['missing'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy         Mean Squared Error (MSE)\n",
      "-----------------------------------------\n",
      "iterative                 18.12\n",
      "most_frequent             30.03\n",
      "median                    28.26\n",
      "mean                      29.04\n"
     ]
    }
   ],
   "source": [
    "# Let us compare how much the imputed values deviate from the original value\n",
    "# We can use mean squared error for instance.\n",
    "# Hmmm, the iterative imputing seemed to have done a better job.\n",
    "\n",
    "print(\"Strategy         Mean Squared Error (MSE)\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "for c in imputedDF.columns:\n",
    "    if c not in ('missing', 'original'):\n",
    "        print('%-15s %15.2f' % (c, np.round(mean_squared_error(imputedDF['original'], imputedDF[c]), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion<a class=\"anchor\" id=\"discussion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, I can not say for sure that which imputation method (univariate vs. multivariate) worked well for this dataset. When I compared the predictions of a randomly picked Rf model, the imputation seems to have no effect. Results are all close. This may due to the Rf algorithm, which is ok with missing data. \n",
    "\n",
    "However, when I look at the imputation results, MSE is the lowest for the multivariate imputation. In other words, multivariate imputation suggest values which are more close the original values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Imputation of missing values, Scikit Learn](https://scikit-learn.org/stable/modules/impute.html#id4)\n",
    "2. [Heart Failure Prediction Dataset](https://www.kaggle.com/fedesoriano/heart-failure-prediction).\n",
    "3. [Statistical Imputation for Missing Values in Machine Learning](https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Disclaimer!** This notebook is available for educational purposes only. There is no guarantee on the correctness of the content provided.\n",
    "\n",
    "If you think there is any copyright violation, please let me [know](https://forms.gle/BNNRB2kR8ZHVEREq8). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
